---
title: "Week 3"
author: "yob"
date: "5/16/2019"
output:
  html_document:
    df_print: paged
---
-------------------

# Week 3 Classes

## Communicating Results

- Specifying Levels of Detail

tl;dr
- people are busy, especially managers and leaders
- results of data analyses are sometimes presented in oral form, but often the first cut is presented via email
-it is often useful to breakdown the results of an analysis into different levels of granularity/detail

### Hierarchy of Information: Research Paper
-Title/ Author list
-abstract
-body/results
-supplementary materials/ the gory details
-code/ data/ really gory details

### Email Presentation

Subject line / Sender info
-at a minimum; include one
-can you summarize findings in one sentence?

Email Body
- a brief description of the problem/context; recall what was proposed and executed; summarize findings/ results; 1- 2 paragraphs
-if action needs to be taken as a result of this presentation, suggest some options and make them as concrete as possible.
-if questions need to be addressed, try to make them yes/no

Attachment(s)
-R markdown file
-knitr report
-stay concise; don't spit out pages of code (because you used knitr we know it's available)

links to supplementary materials
-code/software/data
-github repository/project web site


## RPubs

rpubs.com

create an account, and publish markdown files to the website to share with the public

everything is public, there is no private option.

## Reproducible Research Checklist

### DO : Start with good science

-coherent, focused question simplifies many problems

-working with good collaborators reinforces good practices

-something that's interesting to you will (hopefully) motivate good habits

### DON'T : Do things by hand

-editing spreadsheets of data to "clean it up"
        -removing outliers
        -QA/QC
        -Validating
-Editing tables or figures(e.g. rounding, formatting)
-Downloading data from a web site(clicking links in a web browser)
-Moving data around your computer; splitting/reformatting data files
-"We're just going to do this once..."

Things done by hand needs to be precisely documented(harder than it sounds)


### DON'T: Point and Click
-many data processing/statistical analysis packages have graphical user interfaces(GUIs)
-GUIs are convenient/intuitive but the actions you take with a GUI can be difficult for others to reproduce
-some GUIs produce a log file or script which includes equivalent commands; these can be saved for later examination
-in general, be careful with data analysis software that is highly interactive; ease of use can sometimes lead to non-reproducible analyses
-other inteactive software, such as text editors, are usually fine

### DO: Teach a Computer
-if something needs to be done as part of your analysis/ investigation, try to code your computer to do it(even if you only need to do it once)
-in order to give your computer instructions, you need to write down exactly what you mean to do and how it should be done
-coding a computer almost guarantees reproduciblity

for example, by hand
1. go to the UCI machine learning repository at http:archive.ics.uci.edu/ml/

2. download the bike sharing dataset by clicking on the link to the Data Folder, then clicking on the link to the zip file of dataset, and choosing "save linked file as..." and then saving it to a folder on your computer

### OR

you can code your computer

```{r}
download.file("http://archive.isc.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip", "Bike-Sharing-Dataset.zip")
```

### DO: Use some version control

-slow things down
-add changes in small chunks(don't just do one massive commit)
-track/tag snapshots
-software like GitHub/BitBucket/SourceForge make it easy to publish results

### DO: Keep Track of your Software Environment

-Computer architecture: CPU(intel, AMD, ARM), and GPUs
-Operating System: Windows, Mac OS, Linux/Unix
-Software toolchain: compilers, interpreters, command shell, programming languages(C, Perl, Python, etc), database backends, data analysis software
-Supporting software/infrastructure: libraries, R packages, dependencies
-external dependencies: web sites, data repositories, remote databases, software repositories
-version numbers: ideally, for everything(if available)

```{r}
sessionInfo()
```

### DON'T: Save Output
-aviod saving data analysis output(tables, figures, summaries, processed data, etc) except perhaps temporarily for efficiency purposes
-if a stray output file cannot be easily connected with the means by which it was created, then it is not reproducible
-save the data+code that generated the output, rather than the output itself
-intermediate files are okay as long as there is clear documentation of how they were created

### DO : Set your seed
-random number generators generate pseudo-random numbers based on an initial seed
-in R you can use set.seed() function to set the seed
-setting the seed allows for the stream of random numbers to be exactly reproducible
-whenever you generate random numbers for a non-trivial purpose, **ALWAYS SET THE SEED**

### DO: Think about the entire pipeline
-data analysis is a lengthy process; it is not just tables/figures/reports
-raw data -> processed data -> analysis -> report
-how you get the end is just as important as the end itself
-the more of the data analysis pipeline you can make reproducible, the better for everyone


### Summary: checklist
-are we doing good science?
-was any part of this analysis done by hand?
-have we code a computer to do as much as possible?
-are we using a version control system?
-have we documented our software environment?
-have we saved any output that we cannot reconstruct from original data+code?
-how far back in the analysis pipeline can we go before the results are no longer reproducible?

## Evidence-based Data Analysis

Replication focuses on the test, reproducibility focuses on the validity of the data analysis

arguably a minimum standard for any scientific study

new investigators, same data, same methods

important when replication is impossible


### ihtiyac sebepleri
-some studies cannot be replicated:no time, no money, unique/opportunistic

-technology is increasing data collection throughput: data are more complex and high-dimensional

-existing databases can be merged to become bigger databases(but data are used off-label)

-computer power allows more sophisticated analyses, even on "small" data

-for every field "X" there is a "Computational X"

## Who reproduces research?

-re-run analysis; check results match
-check the code for bugs/errors
-check the sensivity of the analysis


